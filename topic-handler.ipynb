{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topical Modelling Banking Inquiry Hearings\n",
    "\n",
    "I picked the hearings of the Joint Committee of Inquiry into the Banking Crisis because it's a relatively small dataset and because I've previously done some work to add keywords to it, drawn from a list provided by the committee staff. It will be interesting to compare the topic clusters with these keywords.\n",
    "\n",
    "The banking inquiry hearings themselves are derivations from the original daily XML of committee hearings. Whereas the daily XML document contains all the hearings held on an individual date in one document, the hearings XML is divided into one document per hearing (and also including the attendance details though that's not relevant for this exercise). This doesn't make any difference to the workflow of the task, because I'm interested in the speaker uris, which are numbered according to the daily document if I ever want to cross-reference them. I'm only noting it now in case I run into any unexpected issues when I expand this approach to the broader set of debates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Retrieve XML from eXist-db and transfer to MongoDB collection\n",
    "\n",
    "I'm transferring the data from a native XML database, eXist-db, to MongoDB because the latter has a more useful python interface, natively stores json and binary objects (eXist-db does the same but I haven't figured that out yet), and is more memory efficient (or at least I have a better idea how to use it in a memory efficient manner).\n",
    "\n",
    "I'm retrieving speech elements from the hearing XML. The specific fields I want are: \n",
    "\n",
    "* The hearing date in xs:date format (as a string), \n",
    "* the speaker eId;\n",
    "* the speech URI, in the form of the hearing URI appended with the parent debateSection eId followed by the speech eId;\n",
    "* the text of the speech itself, as an array of strings of text for each paragraph; and\n",
    "* the MongoDB ``_id`` for the speech document, which is the speech URI with the substring _akn/ie/debateRecord_ replaced with _speech_\n",
    "\n",
    "The data is retrieved in json format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy.tokens.doc import Doc\n",
    "from spacy.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9 µs, sys: 2 µs, total: 11 µs\n",
      "Wall time: 11 µs\n",
      "\n",
      "eXist-db http request status code: 200\n",
      "Number of documents retrieved: 60067\n",
      "\n",
      "---------------------------------------\n",
      "\n",
      "Example speech document\n",
      "\n",
      "{'_id': '/speech/joint-committee-of-inquiry-into-the-banking-crisis/2014-12-17/eng@hearing_1/dbsect_2/spk_2',\n",
      " 'date': '2014-12-17',\n",
      " 'spkr': 'person/ie/oireachtas/committee/inquiry-into-the-banking-crisis/witness/peter.nyberg',\n",
      " 'text': ['Paragraph', 'Another paragraph'],\n",
      " 'uri': '/akn/ie/debateRecord/joint-committee-of-inquiry-into-the-banking-crisis/2014-12-17/eng@hearing_1/dbsect_2/spk_2'}\n",
      "\n",
      "---------------------------------------\n",
      "\n",
      "MongoDB insert many success: True\n",
      "No. of documents in MongoDB collection: 60067\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "db.drop_collection(\"banking\")\n",
    "%run exist-to-mongo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Speeches with Spacy\n",
    "\n",
    "I'm loading the full spacy English parser, including dependency parser and entity tagger, although for the topic modelling I only require the tokenizer and POS parser. I'm trying this approach first on the basis that I'll be working with sentence dependency trees later so might as well get the whole lot out of the way first. This may not be worthwhile when I come to the full project if I only examine dependency structures for a subset of debates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 32s, sys: 3.02 s, total: 1min 35s\n",
      "Wall time: 2min 26s\n"
     ]
    }
   ],
   "source": [
    "%time nlp = spacy.en.English()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each text array in the MongoDb collection is merged into one long string and processed with spacy to produce an output array of lemmas with stop words and non-alphabet words removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_doc(text):\n",
    "    doc = nlp(text)\n",
    "    toks = [w.lemma_ for w in doc if w.is_alpha and not w.is_stop]\n",
    "    return doc, toks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding processed text to MongoDB as I go so I can come back to it later. This was a computationally expensive exercise, using up 96%-100% of CPU but only 36% of memory, about the amount used by spacy on its own. I only timed the tokenize_doc function (mostly in the 1-5 ms range) but didn't time the entire process - it took about 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "speeches = db.banking.find()\n",
    "for speech in speeches:\n",
    "    text = \"\".join(speech[\"text\"])\n",
    "    #print(\"String length of speech: {}\".format(len(text)))\n",
    "    %time doc, toks = tokenize_doc(text)\n",
    "    #print(\"Number of tokens in tokenized doc: {}\".format(len(doc)))\n",
    "    #print(\"Number of tokens after processing: {}\".format(len(toks)))\n",
    "    up = db.banking.update_one({\"_id\":speech[\"_id\"]}, {\"$set\":{\"byte_string\":doc.to_bytes(), \"tokens\":toks}})\n",
    "    #print(up.acknowledged)\n",
    "    #print(\"\\n---------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have the byte_strings, it's very speedy to do things like identify entities (even if they're not always accurate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Financial Regulator', 'Committee', 'Finnish Ministry of Finance', 'Washington', 'OECD', 'Cyprus', 'DSG', 'United States', 'Irish Government', 'America', 'Houses of Oireachtas on 17 October', 'ECB', 'National Asset Management Agency', 'Finland', 'Germany', 'Anglo Irish Bank', 'EU', 'Financial Regulator \"', 'NAMA', '€26', 'RTE', 'Central Bank', 'Credit Institutions (Financial Support', 'Ministry', 'Australia', 'Scandinavia', 'Department of Finance', 'United State', 'EU Commission', 'Joint Committee of Inquiry', 'Parliament', 'NTMA', 'ESB', 'IMF', 'Act', 'USA', 'Government', 'Garda', 'Canada', 'The Department of Finance', 'Iceland', 'Commission', 'Houses of Oireachtas', 'Banking Crisis', 'PwC', 'The National Treasury Management Agency', 'Defamation Act 2009', 'Ireland', 'State', 'Banking Sector', 'UK', 'European Union', 'Bank of Finland', 'Department', 'Nyberg', 'Anglo', 'Irish Central Bank', 'Spain', 'Irish Nationwide Building Society', 'European Central Bank', 'US', 'Office of Financial Regulator', 'Irish Nationwide', 'Houses', 'Honan'}\n"
     ]
    }
   ],
   "source": [
    "taster = db.banking.find({\"date\":\"2014-12-17\"})\n",
    "ent_list = []\n",
    "for t in taster:\n",
    "    doc = Doc(nlp.vocab).from_bytes(t['byte_string'])\n",
    "    ent_list.extend([e.orth_.rstrip(\".\").replace(\"the \", \"\").replace(\"'s\", \"\") for e in doc.ents if e.label_==\"ORG\" or e.label_ ==\"GPE\"])\n",
    "print(set(ent_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It has not.  It is the same.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['it', 'have', 'not', '.', ' ', 'it', 'be', 'the', 'same', '.']"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = Doc(nlp.vocab).from_bytes(db.banking.find_one()['byte_string'])\n",
    "print(d)\n",
    "[w.lemma_ for w in d]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modelling with Gensim\n",
    "\n",
    "For this part of the evaluation, I'm following as closely as possible the [Gensim tutorial](https://radimrehurek.com/gensim/tutorial.html). The banking inquiry corpus might well fit in memory but I'm trying the streaming approach because I will need to scale up. The topic clusters are visualised in this [notebook](LDA_viz.ipynb#topic=&lambda=0.41&term=) or in the [browser](banking_lda.html#topic=&lambda=0.6&term=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Ignoring warnings because of this issue https://github.com/bmabey/pyLDAvis/issues/8\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BankingCorpus(object):\n",
    "    def __iter__(self):\n",
    "        for speech in db.banking.find({}, {\"tokens\":True}):\n",
    "            yield dictionary.doc2bow(speech['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.19 s, sys: 59.9 ms, total: 3.25 s\n",
      "Wall time: 3.36 s\n",
      "CPU times: user 17 ms, sys: 0 ns, total: 17 ms\n",
      "Wall time: 16.9 ms\n",
      "Dictionary(9886 unique tokens: ['', 'composition', 'secretly', 'instantly', 'mobilisation']...)\n"
     ]
    }
   ],
   "source": [
    "speeches = db.banking.find({}, {\"tokens\":True})\n",
    "# collect statistics about all tokens\n",
    "%time dictionary = corpora.Dictionary(speech['tokens'] for speech in speeches)\n",
    "#filter out words that only appear once\n",
    "once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq == 1]\n",
    "%time dictionary.filter_tokens(once_ids)\n",
    "dictionary.compactify() # remove gaps in id sequence after words that were removed\n",
    "dictionary.save('banking.dict') # store the dictionary, for future reference\n",
    "print(dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.05 s, sys: 27.9 ms, total: 3.08 s\n",
      "Wall time: 3.18 s\n"
     ]
    }
   ],
   "source": [
    "corpus = BankingCorpus()\n",
    "%time banking_corpus = [v for v in corpus]\n",
    "corpora.MmCorpus.serialize('banking_corpus.mm', banking_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.4 s, sys: 0 ns, total: 3.4 s\n",
      "Wall time: 3.22 s\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 18.6 µs\n"
     ]
    }
   ],
   "source": [
    "banking_corpus = corpora.MmCorpus('banking_corpus.mm')\n",
    "%time tfidf = models.TfidfModel(banking_corpus) # step 1 -- initialize a model\n",
    "%time corpus_tfidf = tfidf[banking_corpus]\n",
    "corpora.MmCorpus.serialize('banking_tfidf_corpus.mm', corpus_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 50.3 s, sys: 51.8 ms, total: 50.4 s\n",
      "Wall time: 50.3 s\n"
     ]
    }
   ],
   "source": [
    "%time lda_model = models.ldamodel.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(11,\n",
       "  '0.028*proposal + 0.027*repay + 0.025*chairman + 0.023*sharing + 0.022*alternative + 0.020*right + 0.020*sure + 0.018*asset + 0.017*fully + 0.015*procedure'),\n",
       " (8,\n",
       "  '0.020*burden + 0.019*share + 0.018*relation + 0.017*cardiff + 0.016*reduce + 0.015*minister + 0.015*noonan + 0.013*cabinet + 0.013*kevin + 0.013*meeting'),\n",
       " (22,\n",
       "  '0.011*house + 0.011*know + 0.011*price + 0.009*sell + 0.009*recall + 0.008*statutory + 0.008*think + 0.008*limit + 0.007*prepare + 0.007*lot'),\n",
       " (24,\n",
       "  '0.017*reduction + 0.013*realise + 0.013*hazard + 0.013*exception + 0.012*walk + 0.012*report + 0.012*importance + 0.012*head + 0.012*successful + 0.012*instance')]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topics(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_model.save(\"banking_lda.lda\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
